{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58247fa5",
   "metadata": {},
   "source": [
    "# 自己的情感分析任务\n",
    "针对自己的数据集的情感分析任务，加载之前训练好的预训练模型，用自己的游记文本数据再次训练模型，学习游记文本中的语义\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e86045",
   "metadata": {},
   "source": [
    "# 预处理自己的游记文本数据集\n",
    "对标注好的游记文本数据进行打乱处理，划分好训练集、验证集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8853e2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as D\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fcf67c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_dataset in module datasets.load:\n",
      "\n",
      "load_dataset(path: str, name: Union[str, NoneType] = None, data_dir: Union[str, NoneType] = None, data_files: Union[str, Sequence[str], Mapping[str, Union[str, Sequence[str]]], NoneType] = None, split: Union[str, datasets.splits.Split, NoneType] = None, cache_dir: Union[str, NoneType] = None, features: Union[datasets.features.features.Features, NoneType] = None, download_config: Union[datasets.download.download_config.DownloadConfig, NoneType] = None, download_mode: Union[datasets.download.download_manager.DownloadMode, NoneType] = None, ignore_verifications: bool = False, keep_in_memory: Union[bool, NoneType] = None, save_infos: bool = False, revision: Union[str, datasets.utils.version.Version, NoneType] = None, use_auth_token: Union[bool, str, NoneType] = None, task: Union[str, datasets.tasks.base.TaskTemplate, NoneType] = None, streaming: bool = False, **config_kwargs) -> Union[datasets.dataset_dict.DatasetDict, datasets.arrow_dataset.Dataset, datasets.dataset_dict.IterableDatasetDict, datasets.iterable_dataset.IterableDataset]\n",
      "    Load a dataset from the Hugging Face Hub, or a local dataset.\n",
      "    \n",
      "    You can find the list of datasets on the Hub at https://huggingface.co/datasets or with ``datasets.list_datasets()``.\n",
      "    \n",
      "    A dataset is a directory that contains:\n",
      "    \n",
      "    - some data files in generic formats (JSON, CSV, Parquet, text, etc.)\n",
      "    - and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.\n",
      "    \n",
      "    Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.\n",
      "    \n",
      "    This function does the following under the hood:\n",
      "    \n",
      "        1. Download and import in the library the dataset script from ``path`` if it's not already cached inside the library.\n",
      "    \n",
      "            If the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)\n",
      "    \n",
      "            Dataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,\n",
      "            contain the path or URL to the original data files and the code to load examples from the original data files.\n",
      "    \n",
      "            You can find some of the scripts here: https://github.com/huggingface/datasets/tree/main/datasets\n",
      "            You can find the complete list of datasets in the Datasets Hub at https://huggingface.co/datasets\n",
      "    \n",
      "        2. Run the dataset script which will:\n",
      "    \n",
      "            * Download the dataset file from the original URL (see the script) if it's not already available locally or cached.\n",
      "            * Process and cache the dataset in typed Arrow tables for caching.\n",
      "    \n",
      "                Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\n",
      "                They can be directly accessed from disk, loaded in RAM or even streamed over the web.\n",
      "    \n",
      "        3. Return a dataset built from the requested splits in ``split`` (default: all).\n",
      "    \n",
      "    It also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script.\n",
      "    In this case, it automatically loads all the data files from the directory or the dataset repository.\n",
      "    \n",
      "    Args:\n",
      "    \n",
      "        path (:obj:`str`): Path or name of the dataset.\n",
      "            Depending on ``path``, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.\n",
      "    \n",
      "            For local datasets:\n",
      "    \n",
      "            - if ``path`` is a local directory (containing data files only)\n",
      "              -> load a generic dataset builder (csv, json, text etc.) based on the content of the directory\n",
      "              e.g. ``'./path/to/directory/with/my/csv/data'``.\n",
      "            - if ``path`` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory):\n",
      "              -> load the dataset builder from the dataset script\n",
      "              e.g. ``'./dataset/squad'`` or ``'./dataset/squad/squad.py'``.\n",
      "    \n",
      "            For datasets on the Hugging Face Hub (list all available datasets and ids with ``datasets.list_datasets()``)\n",
      "    \n",
      "            - if ``path`` is a dataset repository on the HF hub (containing data files only)\n",
      "              -> load a generic dataset builder (csv, text etc.) based on the content of the repository\n",
      "              e.g. ``'username/dataset_name'``, a dataset repository on the HF hub containing your data files.\n",
      "            - if ``path`` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)\n",
      "              -> load the dataset builder from the dataset script in the dataset repository\n",
      "              e.g. ``glue``, ``squad``, ``'username/dataset_name'``, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.\n",
      "    \n",
      "        name (:obj:`str`, optional): Defining the name of the dataset configuration.\n",
      "        data_dir (:obj:`str`, optional): Defining the data_dir of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is None,\n",
      "            the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\n",
      "        data_files (:obj:`str` or :obj:`Sequence` or :obj:`Mapping`, optional): Path(s) to source data file(s).\n",
      "        split (:class:`Split` or :obj:`str`): Which split of the data to load.\n",
      "            If None, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\n",
      "            If given, will return a single Dataset.\n",
      "            Splits can be combined and specified like in tensorflow-datasets.\n",
      "        cache_dir (:obj:`str`, optional): Directory to read/write data. Defaults to \"~/.cache/huggingface/datasets\".\n",
      "        features (:class:`Features`, optional): Set the features type to use for this dataset.\n",
      "        download_config (:class:`~utils.DownloadConfig`, optional): Specific download configuration parameters.\n",
      "        download_mode (:class:`DownloadMode`, default ``REUSE_DATASET_IF_EXISTS``): Download/generate mode.\n",
      "        ignore_verifications (:obj:`bool`, default ``False``): Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...).\n",
      "        keep_in_memory (:obj:`bool`, default ``None``): Whether to copy the dataset in-memory. If `None`, the dataset\n",
      "            will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\n",
      "            nonzero. See more details in the :ref:`load_dataset_enhancing_performance` section.\n",
      "        save_infos (:obj:`bool`, default ``False``): Save the dataset information (checksums/size/splits/...).\n",
      "        revision (:class:`~utils.Version` or :obj:`str`, optional): Version of the dataset script to load:\n",
      "    \n",
      "            - For datasets in the `huggingface/datasets` library on GitHub like \"squad\", the default version of the module is the local version of the lib.\n",
      "              You can specify a different version from your local version of the lib (e.g. \"main\" or \"1.2.0\") but it might cause compatibility issues.\n",
      "            - For community datasets like \"lhoestq/squad\" that have their own git repository on the Datasets Hub, the default version \"main\" corresponds to the \"main\" branch.\n",
      "              You can specify a different version that the default \"main\" by using a commit sha or a git tag of the dataset repository.\n",
      "        use_auth_token (``str`` or :obj:`bool`, optional): Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\n",
      "            If True, will get token from `\"~/.huggingface\"`.\n",
      "        task (``str``): The task to prepare the dataset for during training and evaluation. Casts the dataset's :class:`Features` to standardized column names and types as detailed in :py:mod:`datasets.tasks`.\n",
      "        streaming (:obj:`bool`, default ``False``): If set to True, don't download the data files. Instead, it streams the data progressively while\n",
      "            iterating on the dataset. An IterableDataset or IterableDatasetDict is returned instead in this case.\n",
      "    \n",
      "            Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\n",
      "            Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\n",
      "            like rar and xz are not yet supported. The tgz format doesn't allow streaming.\n",
      "        **config_kwargs (additional keyword arguments): Keyword arguments to be passed to the :class:`BuilderConfig`\n",
      "            and used in the :class:`DatasetBuilder`.\n",
      "    \n",
      "    Returns:\n",
      "        :class:`Dataset` or :class:`DatasetDict`:\n",
      "        - if `split` is not None: the dataset requested,\n",
      "        - if `split` is None, a ``datasets.DatasetDict`` with each split.\n",
      "    \n",
      "        or :class:`IterableDataset` or :class:`IterableDatasetDict`: if streaming=True\n",
      "    \n",
      "        - if `split` is not None: the dataset requested,\n",
      "        - if `split` is None, a ``datasets.streaming.IterableDatasetDict`` with each split.\n",
      "    \n",
      "    <Tip>\n",
      "    \n",
      "    Passing `use_auth_token=True` is required when you want to access a private dataset.\n",
      "    \n",
      "    </Tip>\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    Load a dataset from the Hugging Face Hub:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('rotten_tomatoes', split='train')\n",
      "    \n",
      "    # Map data files to splits\n",
      "    >>> data_files = {'train': 'train.csv', 'test': 'test.csv'}\n",
      "    >>> ds = load_dataset('namespace/your_dataset_name', data_files=data_files)\n",
      "    ```\n",
      "    \n",
      "    Load a local dataset:\n",
      "    \n",
      "    ```py\n",
      "    # Load a CSV file\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')\n",
      "    \n",
      "    # Load a JSON file\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('json', data_files='path/to/local/my_dataset.json')\n",
      "    \n",
      "    # Load from a local loading script\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')\n",
      "    ```\n",
      "    \n",
      "    Load an [`~datasets.IterableDataset`]:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('rotten_tomatoes', split='train', streaming=True)\n",
      "    ```\n",
      "    \n",
      "    Load an image dataset with the `ImageFolder` dataset builder:\n",
      "    \n",
      "    ```py\n",
      "    >>> from datasets import load_dataset\n",
      "    >>> ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(load_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8946003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4d90218a9b3a1753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/chenli/.cache/huggingface/datasets/csv/default-4d90218a9b3a1753/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e0d76b02114446a029033c0beb1731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408be9a2795b4e4c95462a35db676f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/chenli/.cache/huggingface/datasets/csv/default-4d90218a9b3a1753/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7130a226f2194fccbf73c50cbcc29246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('csv',data_files='../data/MyDataset/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0705a5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2445\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db2c35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_dataset = dataset.shuffle(9) # 将数据打乱，数值越大，混乱程度越大"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62ef0c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(shuffle_dataset))\n",
    "valid_size = int(0.1 * len(shuffle_dataset))\n",
    "test_size = len(shuffle_dataset) - train_size - valid_size\n",
    "train_dataset,valid_dataset,test_dataset = torch.utils.data.random_split(shuffle_dataset, [train_size,valid_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bc19be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_size)\n",
    "print(valid_size)\n",
    "print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a068b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc684d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5597ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

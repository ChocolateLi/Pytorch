{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "868f71c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenli/anaconda3/envs/pytorch/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 247kB/s]  \n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 25.6kB/s]\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 454kB/s]\n",
      "Downloading: 100%|██████████| 440M/440M [05:27<00:00, 1.35MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc0248a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beb52567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape # 最后一层的维度情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78e994f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.shape # 最后一层的第一个CLS的维度情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c783c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(outputs.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "329f8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5118c9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 1, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "x3 = x.squeeze(2)\n",
    "print(x3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f107edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "x4 = x.squeeze(3)\n",
    "print(x4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d86455c",
   "metadata": {},
   "source": [
    "# 开始编写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5465cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#定义数据集,方便后续模型读取批量数据。\n",
    "class Dataset(Data.Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        self.data = self.load_data(data_path)\n",
    "    \n",
    "    # 核心要变的就是load_data这部分函数\n",
    "    # 改造成适合自己任务的数据集\n",
    "    def load_data(self, data_path):\n",
    "        # 先加载本地数据集\n",
    "        tmp_dataset = load_dataset('csv',data_files=data_path,split='train')\n",
    "        Data = {}\n",
    "        for idx, line in enumerate(tmp_dataset):\n",
    "            sample = line\n",
    "            Data[idx] = sample\n",
    "        return Data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "887d16a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7edeb8d80da73fdc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/chenli/.cache/huggingface/datasets/csv/default-7edeb8d80da73fdc/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1931.97it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1250.91it/s]\n",
      "                            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/chenli/.cache/huggingface/datasets/csv/default-7edeb8d80da73fdc/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "test_dataset = Dataset('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21fe0ae4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'featurs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeaturs\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'featurs'"
     ]
    }
   ],
   "source": [
    "test_dataset.featurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1b5e524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7edeb8d80da73fdc\n",
      "Found cached dataset csv (/home/chenli/.cache/huggingface/datasets/csv/default-7edeb8d80da73fdc/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    }
   ],
   "source": [
    "tmp_dataset = load_dataset('csv',data_files='./test.csv',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3cf61bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a358d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2214e7b2",
   "metadata": {},
   "source": [
    "# 情感分类模型\n",
    "trainer的使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531816f",
   "metadata": {},
   "source": [
    "## 加载分词工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08879b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载分词工具\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa73e08",
   "metadata": {},
   "source": [
    "## 定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96f44be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 9600\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "datasets = load_from_disk('../SentimentAnalysis/model/data/ChnSentiCorp')\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ed889fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词\n",
    "def tokenizer_function(data):\n",
    "    return tokenizer(data['text'],padding='max_length',truncation=True,max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1399666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2400/2400 [00:07<00:00, 305.82ba/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 303.82ba/s]\n",
      "100%|██████████| 300/300 [00:00<00:00, 302.68ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9600\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = datasets.map(tokenizer_function,batched=True,batch_size=4)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1255052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取数据子集，否则数据太多跑不动\n",
    "dataset_train = datasets['train'].shuffle().select(range(1000))\n",
    "dataset_test = datasets['validation'].shuffle().select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1263e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77768f8e",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "738d6c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10226.9186\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-chinese',num_labels=2)\n",
    "print(sum([i.nelement() for i in model.parameters()])/10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e52049ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total param: 102269186\n"
     ]
    }
   ],
   "source": [
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(\"total param:\",total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e25a9d2",
   "metadata": {},
   "source": [
    "## 定义评价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9ce4b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from transformers.trainer_utils import EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cfd46e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric('glue','mrpc')\n",
    "    logits,labels = eval_preds # 预测值和真实值\n",
    "    predictions = np.argmax(logits,axis=-1)\n",
    "    return metric.compute(predictions=predictions,references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "adfa9751",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/2.5.2/metrics/glue/glue.py (ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=100)\")))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 模拟测试输出\u001b[39;00m\n\u001b[1;32m      2\u001b[0m eval_pred \u001b[38;5;241m=\u001b[39m EvalPrediction(\n\u001b[1;32m      3\u001b[0m     predictions\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m],[\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m],[\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m7\u001b[39m]]),\n\u001b[1;32m      4\u001b[0m     label_ids\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m \u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_pred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_preds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(eval_preds):\n\u001b[0;32m----> 2\u001b[0m     metric \u001b[38;5;241m=\u001b[39m \u001b[43mload_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mglue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmrpc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     logits,labels \u001b[38;5;241m=\u001b[39m eval_preds \u001b[38;5;66;03m# 预测值和真实值\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(logits,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/datasets/utils/deprecation_utils.py:38\u001b[0m, in \u001b[0;36mdeprecated.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(warning_msg, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     37\u001b[0m     _emitted_deprecation_warnings\u001b[38;5;241m.\u001b[39madd(func_hash)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeprecated_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/datasets/load.py:1336\u001b[0m, in \u001b[0;36mload_metric\u001b[0;34m(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, **metric_init_kwargs)\u001b[0m\n\u001b[1;32m   1333\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.*https://huggingface.co/docs/evaluate$\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m   1335\u001b[0m download_mode \u001b[38;5;241m=\u001b[39m DownloadMode(download_mode \u001b[38;5;129;01mor\u001b[39;00m DownloadMode\u001b[38;5;241m.\u001b[39mREUSE_DATASET_IF_EXISTS)\n\u001b[0;32m-> 1336\u001b[0m metric_module \u001b[38;5;241m=\u001b[39m \u001b[43mmetric_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\n\u001b[1;32m   1338\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmodule_path\n\u001b[1;32m   1339\u001b[0m metric_cls \u001b[38;5;241m=\u001b[39m import_main_class(metric_module, dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1340\u001b[0m metric \u001b[38;5;241m=\u001b[39m metric_cls(\n\u001b[1;32m   1341\u001b[0m     config_name\u001b[38;5;241m=\u001b[39mconfig_name,\n\u001b[1;32m   1342\u001b[0m     process_id\u001b[38;5;241m=\u001b[39mprocess_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetric_init_kwargs,\n\u001b[1;32m   1348\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/datasets/utils/deprecation_utils.py:38\u001b[0m, in \u001b[0;36mdeprecated.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(warning_msg, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     37\u001b[0m     _emitted_deprecation_warnings\u001b[38;5;241m.\u001b[39madd(func_hash)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeprecated_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/datasets/load.py:1267\u001b[0m, in \u001b[0;36mmetric_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e2:  \u001b[38;5;66;03m# noqa: if it's not in the cache, then it doesn't exist.\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[0;32m-> 1267\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1268\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1269\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a metric script at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(combined_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1270\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist on the Hugging Face Hub either.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1271\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/datasets/load.py:1255\u001b[0m, in \u001b[0;36mmetric_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, **download_kwargs)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_relative_path(path) \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1255\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGithubMetricModuleFactory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_modules_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e1:  \u001b[38;5;66;03m# noqa: all the attempts failed, before raising the error we should check if the module is already cached.\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/datasets/load.py:469\u001b[0m, in \u001b[0;36mGithubMetricModuleFactory.get_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    467\u001b[0m revision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevision\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 469\u001b[0m     local_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_loading_script\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     revision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrevision\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/datasets/load.py:463\u001b[0m, in \u001b[0;36mGithubMetricModuleFactory.download_loading_script\u001b[0;34m(self, revision)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download_config\u001b[38;5;241m.\u001b[39mdownload_desc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     download_config\u001b[38;5;241m.\u001b[39mdownload_desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading builder script\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/datasets/utils/file_utils.py:187\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(url_or_filename)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_etag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_etag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_url_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_url_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/datasets/utils/file_utils.py:537\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, download_desc)\u001b[0m\n\u001b[1;32m    535\u001b[0m _raise_if_offline_mode_is_enabled(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to reach \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(head_error)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mConnectionError\u001b[0m: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/2.5.2/metrics/glue/glue.py (ReadTimeout(ReadTimeoutError(\"HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Read timed out. (read timeout=100)\")))"
     ]
    }
   ],
   "source": [
    "# 模拟测试输出\n",
    "eval_pred = EvalPrediction(\n",
    "    predictions=np.array([[0,1],[2,3],[4,5],[6,7]]),\n",
    "    label_ids=np.array([1,1,1,1])\n",
    ")\n",
    "compute_metrics(eval_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d01ccd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.9.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (30.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.1/30.1 MB\u001b[0m \u001b[31m27.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:30\u001b[0m\n",
      "\u001b[?25hCollecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.26.0,>=1.18.5 in /home/chenli/anaconda3/envs/pytorch/lib/python3.9/site-packages (from scipy) (1.23.3)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.1.3-cp39-cp39-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (30.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m30.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:37\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.0.0\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m39.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1304 sha256=32fef8345eccfb67476084b35919d18338f273b9410cac1d1723478768f3bbe0\n",
      "  Stored in directory: /home/chenli/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.1.3 scipy-1.9.3 sklearn-0.0 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scipy sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a3b72",
   "metadata": {},
   "source": [
    "## 定义训练器并测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca007a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments,Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f9f247b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# 初始化训练参数\n",
    "args = TrainingArguments(output_dir='./output_dir',evaluation_strategy='epoch') # evaluation_strategy表示每隔多长时间进行一次测试\n",
    "args.num_train_epoch = 1 # 训练的轮次\n",
    "args.learning_rate = 1e-5 # 学习率\n",
    "args.weight_decay = 1e-2\n",
    "args.per_device_eval_batch_size = 32\n",
    "args.per_device_train_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5897008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TrainingArguments in module transformers.training_args object:\n",
      "\n",
      "class TrainingArguments(builtins.object)\n",
      " |  TrainingArguments(output_dir: str, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, evaluation_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: Optional[str] = 'passive', log_level_replica: Optional[str] = 'passive', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: int = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', save_steps: int = 500, save_total_limit: Optional[int] = None, save_on_each_node: bool = False, no_cuda: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, xpu_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: str = '', dataloader_drop_last: bool = False, eval_steps: Optional[int] = None, dataloader_num_workers: int = 0, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[List[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, sharded_ddp: str = '', fsdp: str = '', fsdp_min_num_params: int = 0, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, deepspeed: Optional[str] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_hf', adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Optional[List[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, dataloader_pin_memory: bool = True, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: bool = False, gradient_checkpointing: bool = False, include_inputs_for_metrics: bool = False, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: Optional[int] = 1800) -> None\n",
      " |  \n",
      " |  TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop\n",
      " |  itself**.\n",
      " |  \n",
      " |  Using [`HfArgumentParser`] we can turn this class into\n",
      " |  [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the\n",
      " |  command line.\n",
      " |  \n",
      " |  Parameters:\n",
      " |      output_dir (`str`):\n",
      " |          The output directory where the model predictions and checkpoints will be written.\n",
      " |      overwrite_output_dir (`bool`, *optional*, defaults to `False`):\n",
      " |          If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`\n",
      " |          points to a checkpoint directory.\n",
      " |      do_train (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used\n",
      " |          by your training/evaluation scripts instead. See the [example\n",
      " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      " |      do_eval (`bool`, *optional*):\n",
      " |          Whether to run evaluation on the validation set or not. Will be set to `True` if `evaluation_strategy` is\n",
      " |          different from `\"no\"`. This argument is not directly used by [`Trainer`], it's intended to be used by your\n",
      " |          training/evaluation scripts instead. See the [example\n",
      " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      " |      do_predict (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's\n",
      " |          intended to be used by your training/evaluation scripts instead. See the [example\n",
      " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      " |      evaluation_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n",
      " |          The evaluation strategy to adopt during training. Possible values are:\n",
      " |  \n",
      " |              - `\"no\"`: No evaluation is done during training.\n",
      " |              - `\"steps\"`: Evaluation is done (and logged) every `eval_steps`.\n",
      " |              - `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
      " |  \n",
      " |      prediction_loss_only (`bool`, *optional*, defaults to `False`):\n",
      " |          When performing evaluation and generating predictions, only returns the loss.\n",
      " |      per_device_train_batch_size (`int`, *optional*, defaults to 8):\n",
      " |          The batch size per GPU/TPU core/CPU for training.\n",
      " |      per_device_eval_batch_size (`int`, *optional*, defaults to 8):\n",
      " |          The batch size per GPU/TPU core/CPU for evaluation.\n",
      " |      gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n",
      " |          Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
      " |  \n",
      " |          <Tip warning={true}>\n",
      " |  \n",
      " |          When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\n",
      " |          evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.\n",
      " |  \n",
      " |          </Tip>\n",
      " |  \n",
      " |      eval_accumulation_steps (`int`, *optional*):\n",
      " |          Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\n",
      " |          left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster but\n",
      " |          requires more memory).\n",
      " |      eval_delay (`float`, *optional*):\n",
      " |          Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n",
      " |          evaluation_strategy.\n",
      " |      learning_rate (`float`, *optional*, defaults to 5e-5):\n",
      " |          The initial learning rate for [`AdamW`] optimizer.\n",
      " |      weight_decay (`float`, *optional*, defaults to 0):\n",
      " |          The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]\n",
      " |          optimizer.\n",
      " |      adam_beta1 (`float`, *optional*, defaults to 0.9):\n",
      " |          The beta1 hyperparameter for the [`AdamW`] optimizer.\n",
      " |      adam_beta2 (`float`, *optional*, defaults to 0.999):\n",
      " |          The beta2 hyperparameter for the [`AdamW`] optimizer.\n",
      " |      adam_epsilon (`float`, *optional*, defaults to 1e-8):\n",
      " |          The epsilon hyperparameter for the [`AdamW`] optimizer.\n",
      " |      max_grad_norm (`float`, *optional*, defaults to 1.0):\n",
      " |          Maximum gradient norm (for gradient clipping).\n",
      " |      num_train_epochs(`float`, *optional*, defaults to 3.0):\n",
      " |          Total number of training epochs to perform (if not an integer, will perform the decimal part percents of\n",
      " |          the last epoch before stopping training).\n",
      " |      max_steps (`int`, *optional*, defaults to -1):\n",
      " |          If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
      " |          In case of using a finite iterable dataset the training may stop before reaching the set number of steps\n",
      " |          when all data is exhausted\n",
      " |      lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n",
      " |          The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n",
      " |      warmup_ratio (`float`, *optional*, defaults to 0.0):\n",
      " |          Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
      " |      warmup_steps (`int`, *optional*, defaults to 0):\n",
      " |          Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.\n",
      " |      log_level (`str`, *optional*, defaults to `passive`):\n",
      " |          Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',\n",
      " |          'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and lets the\n",
      " |          application set the level.\n",
      " |      log_level_replica (`str`, *optional*, defaults to `passive`):\n",
      " |          Logger log level to use on replicas. Same choices as `log_level`\"\n",
      " |      log_on_each_node (`bool`, *optional*, defaults to `True`):\n",
      " |          In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n",
      " |          node.\n",
      " |      logging_dir (`str`, *optional*):\n",
      " |          [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\n",
      " |          *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\n",
      " |      logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
      " |          The logging strategy to adopt during training. Possible values are:\n",
      " |  \n",
      " |              - `\"no\"`: No logging is done during training.\n",
      " |              - `\"epoch\"`: Logging is done at the end of each epoch.\n",
      " |              - `\"steps\"`: Logging is done every `logging_steps`.\n",
      " |  \n",
      " |      logging_first_step (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to log and evaluate the first `global_step` or not.\n",
      " |      logging_steps (`int`, *optional*, defaults to 500):\n",
      " |          Number of update steps between two logs if `logging_strategy=\"steps\"`.\n",
      " |      logging_nan_inf_filter (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan`\n",
      " |          or `inf` is filtered and the average loss of the current logging window is taken instead.\n",
      " |  \n",
      " |          <Tip>\n",
      " |  \n",
      " |          `logging_nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n",
      " |          gradient is computed or applied to the model.\n",
      " |  \n",
      " |          </Tip>\n",
      " |  \n",
      " |      save_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
      " |          The checkpoint save strategy to adopt during training. Possible values are:\n",
      " |  \n",
      " |              - `\"no\"`: No save is done during training.\n",
      " |              - `\"epoch\"`: Save is done at the end of each epoch.\n",
      " |              - `\"steps\"`: Save is done every `save_steps`.\n",
      " |      save_steps (`int`, *optional*, defaults to 500):\n",
      " |          Number of updates steps before two checkpoint saves if `save_strategy=\"steps\"`.\n",
      " |      save_total_limit (`int`, *optional*):\n",
      " |          If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
      " |          `output_dir`.\n",
      " |      save_on_each_node (`bool`, *optional*, defaults to `False`):\n",
      " |          When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\n",
      " |          the main one.\n",
      " |  \n",
      " |          This should not be activated when the different nodes use the same storage as the files will be saved with\n",
      " |          the same names for each node.\n",
      " |      no_cuda (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to not use CUDA even when it is available or not.\n",
      " |      seed (`int`, *optional*, defaults to 42):\n",
      " |          Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n",
      " |          [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.\n",
      " |      data_seed (`int`, *optional*):\n",
      " |          Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n",
      " |          same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model\n",
      " |          seed.\n",
      " |      jit_mode_eval (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not to use PyTorch jit trace for inference.\n",
      " |      use_ipex (`bool`, *optional*, defaults to `False`):\n",
      " |          Use Intel extension for PyTorch when it is available. [IPEX\n",
      " |          installation](https://github.com/intel/intel-extension-for-pytorch).\n",
      " |      bf16 (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n",
      " |          NVIDIA architecture or using CPU (no_cuda). This is an experimental API and it may change.\n",
      " |      fp16 (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
      " |      fp16_opt_level (`str`, *optional*, defaults to 'O1'):\n",
      " |          For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on\n",
      " |          the [Apex documentation](https://nvidia.github.io/apex/amp).\n",
      " |      fp16_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
      " |          This argument is deprecated. Use `half_precision_backend` instead.\n",
      " |      half_precision_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
      " |          The backend to use for mixed precision training. Must be one of `\"auto\", \"cuda_amp\", \"apex\", \"cpu_amp\"`.\n",
      " |          `\"auto\"` will use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices\n",
      " |          will force the requested backend.\n",
      " |      bf16_full_eval (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
      " |          metric values. This is an experimental API and it may change.\n",
      " |      fp16_full_eval (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
      " |          metric values.\n",
      " |      tf32 (`bool`, *optional*):\n",
      " |          Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\n",
      " |          on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to\n",
      " |          the [TF32](https://huggingface.co/docs/transformers/performance#tf32) documentation. This is an\n",
      " |          experimental API and it may change.\n",
      " |      local_rank (`int`, *optional*, defaults to -1):\n",
      " |          Rank of the process during distributed training.\n",
      " |      xpu_backend (`str`, *optional*):\n",
      " |          The backend to use for xpu distributed training. Must be one of `\"mpi\"` or `\"ccl\"`.\n",
      " |      tpu_num_cores (`int`, *optional*):\n",
      " |          When training on TPU, the number of TPU cores (automatically passed by launcher script).\n",
      " |      dataloader_drop_last (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n",
      " |          or not.\n",
      " |      eval_steps (`int`, *optional*):\n",
      " |          Number of update steps between two evaluations if `evaluation_strategy=\"steps\"`. Will default to the same\n",
      " |          value as `logging_steps` if not set.\n",
      " |      dataloader_num_workers (`int`, *optional*, defaults to 0):\n",
      " |          Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\n",
      " |          main process.\n",
      " |      past_index (`int`, *optional*, defaults to -1):\n",
      " |          Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of\n",
      " |          the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will\n",
      " |          use the corresponding output (usually index 2) as the past state and feed it to the model at the next\n",
      " |          training step under the keyword argument `mems`.\n",
      " |      run_name (`str`, *optional*):\n",
      " |          A descriptor for the run. Typically used for [wandb](https://www.wandb.com/) and\n",
      " |          [mlflow](https://www.mlflow.org/) logging.\n",
      " |      disable_tqdm (`bool`, *optional*):\n",
      " |          Whether or not to disable the tqdm progress bars and table of metrics produced by\n",
      " |          [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is\n",
      " |          set to warn or lower (default), `False` otherwise.\n",
      " |      remove_unused_columns (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether or not to automatically remove the columns unused by the model forward method.\n",
      " |  \n",
      " |          (Note that this behavior is not implemented for [`TFTrainer`] yet.)\n",
      " |      label_names (`List[str]`, *optional*):\n",
      " |          The list of keys in your dictionary of inputs that correspond to the labels.\n",
      " |  \n",
      " |          Will eventually default to `[\"labels\"]` except if the model used is one of the `XxxForQuestionAnswering` in\n",
      " |          which case it will default to `[\"start_positions\", \"end_positions\"]`.\n",
      " |      load_best_model_at_end (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not to load the best model found during training at the end of training.\n",
      " |  \n",
      " |          <Tip>\n",
      " |  \n",
      " |          When set to `True`, the parameters `save_strategy` needs to be the same as `evaluation_strategy`, and in\n",
      " |          the case it is \"steps\", `save_steps` must be a round multiple of `eval_steps`.\n",
      " |  \n",
      " |          </Tip>\n",
      " |  \n",
      " |      metric_for_best_model (`str`, *optional*):\n",
      " |          Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different\n",
      " |          models. Must be the name of a metric returned by the evaluation with or without the prefix `\"eval_\"`. Will\n",
      " |          default to `\"loss\"` if unspecified and `load_best_model_at_end=True` (to use the evaluation loss).\n",
      " |  \n",
      " |          If you set this value, `greater_is_better` will default to `True`. Don't forget to set it to `False` if\n",
      " |          your metric is better when lower.\n",
      " |      greater_is_better (`bool`, *optional*):\n",
      " |          Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models\n",
      " |          should have a greater metric or not. Will default to:\n",
      " |  \n",
      " |          - `True` if `metric_for_best_model` is set to a value that isn't `\"loss\"` or `\"eval_loss\"`.\n",
      " |          - `False` if `metric_for_best_model` is not set, or set to `\"loss\"` or `\"eval_loss\"`.\n",
      " |      ignore_data_skip (`bool`, *optional*, defaults to `False`):\n",
      " |          When resuming training, whether or not to skip the epochs and batches to get the data loading at the same\n",
      " |          stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step\n",
      " |          can take a long time) but will not yield the same results as the interrupted training would have.\n",
      " |      sharded_ddp (`bool`, `str` or list of [`~trainer_utils.ShardedDDPOption`], *optional*, defaults to `False`):\n",
      " |          Use Sharded DDP training from [FairScale](https://github.com/facebookresearch/fairscale) (in distributed\n",
      " |          training only). This is an experimental feature.\n",
      " |  \n",
      " |          A list of options along the following:\n",
      " |  \n",
      " |          - `\"simple\"`: to use first instance of sharded DDP released by fairscale (`ShardedDDP`) similar to ZeRO-2.\n",
      " |          - `\"zero_dp_2\"`: to use the second instance of sharded DPP released by fairscale (`FullyShardedDDP`) in\n",
      " |            Zero-2 mode (with `reshard_after_forward=False`).\n",
      " |          - `\"zero_dp_3\"`: to use the second instance of sharded DPP released by fairscale (`FullyShardedDDP`) in\n",
      " |            Zero-3 mode (with `reshard_after_forward=True`).\n",
      " |          - `\"offload\"`: to add ZeRO-offload (only compatible with `\"zero_dp_2\"` and `\"zero_dp_3\"`).\n",
      " |  \n",
      " |          If a string is passed, it will be split on space. If a bool is passed, it will be converted to an empty\n",
      " |          list for `False` and `[\"simple\"]` for `True`.\n",
      " |      fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `False`):\n",
      " |          Use PyTorch Distributed Parallel Training (in distributed training only).\n",
      " |  \n",
      " |          A list of options along the following:\n",
      " |  \n",
      " |          - `\"full_shard\"`: Shard parameters, gradients and optimizer states.\n",
      " |          - `\"shard_grad_op\"`: Shard optimizer states and gradients.\n",
      " |          - `\"offload\"`: Offload parameters and gradients to CPUs (only compatible with `\"full_shard\"` and\n",
      " |            `\"shard_grad_op\"`).\n",
      " |          - `\"auto_wrap\"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.\n",
      " |      fsdp_min_num_params (`int`, *optional*, defaults to `0`):\n",
      " |          FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is passed).\n",
      " |      deepspeed (`str` or `dict`, *optional*):\n",
      " |          Use [Deepspeed](https://github.com/microsoft/deepspeed). This is an experimental feature and its API may\n",
      " |          evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n",
      " |          `ds_config.json`) or an already loaded json file as a `dict`\"\n",
      " |      label_smoothing_factor (`float`, *optional*, defaults to 0.0):\n",
      " |          The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\n",
      " |          labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +\n",
      " |          label_smoothing_factor/num_labels` respectively.\n",
      " |      debug (`str` or list of [`~debug_utils.DebugOption`], *optional*, defaults to `\"\"`):\n",
      " |          Enable one or more debug features. This is an experimental feature.\n",
      " |  \n",
      " |          Possible options are:\n",
      " |  \n",
      " |          - `\"underflow_overflow\"`: detects overflow in model's input/outputs and reports the last frames that led to\n",
      " |            the event\n",
      " |          - `\"tpu_metrics_debug\"`: print debug metrics on TPU\n",
      " |  \n",
      " |          The options should be separated by whitespaces.\n",
      " |      optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_hf\"`):\n",
      " |          The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor.\n",
      " |      adafactor (`bool`, *optional*, defaults to `False`):\n",
      " |          This argument is deprecated. Use `--optim adafactor` instead.\n",
      " |      group_by_length (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not to group together samples of roughly the same length in the training dataset (to minimize\n",
      " |          padding applied and be more efficient). Only useful if applying dynamic padding.\n",
      " |      length_column_name (`str`, *optional*, defaults to `\"length\"`):\n",
      " |          Column name for precomputed lengths. If the column exists, grouping by length will use these values rather\n",
      " |          than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an\n",
      " |          instance of `Dataset`.\n",
      " |      report_to (`str` or `List[str]`, *optional*, defaults to `\"all\"`):\n",
      " |          The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n",
      " |          `\"comet_ml\"`, `\"mlflow\"`, `\"neptune\"`, `\"tensorboard\"` and `\"wandb\"`. Use `\"all\"` to report to all\n",
      " |          integrations installed, `\"none\"` for no integrations.\n",
      " |      ddp_find_unused_parameters (`bool`, *optional*):\n",
      " |          When using distributed training, the value of the flag `find_unused_parameters` passed to\n",
      " |          `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
      " |      ddp_bucket_cap_mb (`int`, *optional*):\n",
      " |          When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.\n",
      " |      dataloader_pin_memory (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether you want to pin memory in data loaders or not. Will default to `True`.\n",
      " |      skip_memory_metrics (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\n",
      " |          down the training and evaluation speed.\n",
      " |      push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not to push the model to the Hub every time the model is saved. If this is activated,\n",
      " |          `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`) and the content\n",
      " |          will be pushed each time a save is triggered (depending on your `save_strategy`). Calling\n",
      " |          [`~Trainer.save_model`] will also trigger a push.\n",
      " |  \n",
      " |          <Tip warning={true}>\n",
      " |  \n",
      " |          If `output_dir` exists, it needs to be a local clone of the repository to which the [`Trainer`] will be\n",
      " |          pushed.\n",
      " |  \n",
      " |          </Tip>\n",
      " |  \n",
      " |      resume_from_checkpoint (`str`, *optional*):\n",
      " |          The path to a folder with a valid checkpoint for your model. This argument is not directly used by\n",
      " |          [`Trainer`], it's intended to be used by your training/evaluation scripts instead. See the [example\n",
      " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      " |      hub_model_id (`str`, *optional*):\n",
      " |          The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n",
      " |          which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\n",
      " |          for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of with\n",
      " |          `\"organization_name/model\"`. Will default to `user_name/output_dir_name` with *output_dir_name* being the\n",
      " |          name of `output_dir`.\n",
      " |  \n",
      " |          Will default to the name of `output_dir`.\n",
      " |      hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n",
      " |          Defines the scope of what is pushed to the Hub and when. Possible values are:\n",
      " |  \n",
      " |          - `\"end\"`: push the model, its configuration, the tokenizer (if passed along to the [`Trainer`]) and a\n",
      " |            draft of a model card when the [`~Trainer.save_model`] method is called.\n",
      " |          - `\"every_save\"`: push the model, its configuration, the tokenizer (if passed along to the [`Trainer`]) and\n",
      " |            a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n",
      " |            training, and in case the save are very frequent, a new push is only attempted if the previous one is\n",
      " |            finished. A last push is made with the final model at the end of training.\n",
      " |          - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n",
      " |            last-checkpoint, allowing you to resume training easily with\n",
      " |            `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n",
      " |          - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the output\n",
      " |            folder (so you will get one checkpoint folder per folder in your final repository)\n",
      " |  \n",
      " |      hub_token (`str`, *optional*):\n",
      " |          The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n",
      " |          `huggingface-cli login`.\n",
      " |      hub_private_repo (`bool`, *optional*, defaults to `False`):\n",
      " |          If True, the Hub repo will be set to private.\n",
      " |      gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n",
      " |          If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
      " |      include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not the inputs will be passed to the `compute_metrics` function. This is intended for metrics\n",
      " |          that need inputs, predictions and references for scoring calculation in Metric class.\n",
      " |      auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n",
      " |          Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding\n",
      " |          CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n",
      " |      full_determinism (`bool`, *optional*, defaults to `False`)\n",
      " |          If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in\n",
      " |          distributed training\n",
      " |      torchdynamo (`str`, *optional*):\n",
      " |          The token that is used to set the backend compiler for TorchDynamo. Possible choices are [\"eager\",\n",
      " |          \"nvfuser]. This is an experimental API and subject to change.\n",
      " |      ray_scope (`str`, *optional*, defaults to `\"last\"`):\n",
      " |          The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray will\n",
      " |          then use the last checkpoint of all trials, compare those, and select the best one. However, other options\n",
      " |          are also available. See the [Ray documentation](\n",
      " |          https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for\n",
      " |          more options.\n",
      " |      ddp_timeout (`int`, *optional*, defaults to 1800):\n",
      " |          The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when\n",
      " |          performing slow operations in distributed runnings. Please refer the [PyTorch documentation]\n",
      " |          (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more\n",
      " |          information.\n",
      " |      use_mps_device (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to use Apple Silicon chip based `mps` device.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |  \n",
      " |  __init__(self, output_dir: str, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, evaluation_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: Optional[str] = 'passive', log_level_replica: Optional[str] = 'passive', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: int = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', save_steps: int = 500, save_total_limit: Optional[int] = None, save_on_each_node: bool = False, no_cuda: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, xpu_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: str = '', dataloader_drop_last: bool = False, eval_steps: Optional[int] = None, dataloader_num_workers: int = 0, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[List[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, sharded_ddp: str = '', fsdp: str = '', fsdp_min_num_params: int = 0, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, deepspeed: Optional[str] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_hf', adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Optional[List[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, dataloader_pin_memory: bool = True, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: bool = False, gradient_checkpointing: bool = False, include_inputs_for_metrics: bool = False, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: Optional[int] = 1800) -> None\n",
      " |  \n",
      " |  __post_init__(self)\n",
      " |  \n",
      " |  __repr__ = __str__(self)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  get_process_log_level(self)\n",
      " |      Returns the log level to be used depending on whether this process is the main process of node 0, main process\n",
      " |      of node non-0, or a non-main process.\n",
      " |      \n",
      " |      For the main process the log level defaults to `logging.INFO` unless overridden by `log_level` argument.\n",
      " |      \n",
      " |      For the replica processes the log level defaults to `logging.WARNING` unless overridden by `log_level_replica`\n",
      " |      argument.\n",
      " |      \n",
      " |      The choice between the main and replica process settings is made according to the return value of `should_log`.\n",
      " |  \n",
      " |  get_warmup_steps(self, num_training_steps: int)\n",
      " |      Get number of steps used for a linear warmup.\n",
      " |  \n",
      " |  main_process_first(self, local=True, desc='work')\n",
      " |      A context manager for torch distributed environment where on needs to do something on the main process, while\n",
      " |      blocking replicas, and when it's finished releasing the replicas.\n",
      " |      \n",
      " |      One such use is for `datasets`'s `map` feature which to be efficient should be run once on the main process,\n",
      " |      which upon completion saves a cached version of results and which then automatically gets loaded by the\n",
      " |      replicas.\n",
      " |      \n",
      " |      Args:\n",
      " |          local (`bool`, *optional*, defaults to `True`):\n",
      " |              if `True` first means process of rank 0 of each node if `False` first means process of rank 0 of node\n",
      " |              rank 0 In multi-node environment with a shared filesystem you most likely will want to use\n",
      " |              `local=False` so that only the main process of the first node will do the processing. If however, the\n",
      " |              filesystem is not shared, then the main process of each node will need to do the processing, which is\n",
      " |              the default behavior.\n",
      " |          desc (`str`, *optional*, defaults to `\"work\"`):\n",
      " |              a work description to be used in debug logs\n",
      " |  \n",
      " |  to_dict(self)\n",
      " |      Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
      " |      the token values by removing their value.\n",
      " |  \n",
      " |  to_json_string(self)\n",
      " |      Serializes this instance to a JSON string.\n",
      " |  \n",
      " |  to_sanitized_dict(self) -> Dict[str, Any]\n",
      " |      Sanitized serialization to use with TensorBoard’s hparams\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  ddp_timeout_delta\n",
      " |      The actual timeout for torch.distributed.init_process_group since it expects a timedelta variable.\n",
      " |  \n",
      " |  device\n",
      " |      The device used by this process.\n",
      " |  \n",
      " |  eval_batch_size\n",
      " |      The actual batch size for evaluation (may differ from `per_gpu_eval_batch_size` in distributed training).\n",
      " |  \n",
      " |  local_process_index\n",
      " |      The index of the local process used.\n",
      " |  \n",
      " |  n_gpu\n",
      " |      The number of GPUs used by this process.\n",
      " |      \n",
      " |      Note:\n",
      " |          This will only be greater than one when you have multiple GPUs available but are not using distributed\n",
      " |          training. For distributed training, it will always be 1.\n",
      " |  \n",
      " |  parallel_mode\n",
      " |      The current mode used for parallelism if multiple GPUs/TPU cores are available. One of:\n",
      " |      \n",
      " |      - `ParallelMode.NOT_PARALLEL`: no parallelism (CPU or one GPU).\n",
      " |      - `ParallelMode.NOT_DISTRIBUTED`: several GPUs in one single process (uses `torch.nn.DataParallel`).\n",
      " |      - `ParallelMode.DISTRIBUTED`: several GPUs, each having its own process (uses\n",
      " |        `torch.nn.DistributedDataParallel`).\n",
      " |      - `ParallelMode.TPU`: several TPU cores.\n",
      " |  \n",
      " |  place_model_on_device\n",
      " |      Can be subclassed and overridden for some specific integrations.\n",
      " |  \n",
      " |  process_index\n",
      " |      The index of the current process used.\n",
      " |  \n",
      " |  should_log\n",
      " |      Whether or not the current process should produce log.\n",
      " |  \n",
      " |  should_save\n",
      " |      Whether or not the current process should write to disk, e.g., to save models and checkpoints.\n",
      " |  \n",
      " |  train_batch_size\n",
      " |      The actual batch size for training (may differ from `per_gpu_train_batch_size` in distributed training).\n",
      " |  \n",
      " |  world_size\n",
      " |      The number of processes used in parallel.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'_n_gpu': <class 'int'>, 'adafactor': <class 'bool'...\n",
      " |  \n",
      " |  __dataclass_fields__ = {'_n_gpu': Field(name='_n_gpu',type=<class 'int...\n",
      " |  \n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |  \n",
      " |  __hash__ = None\n",
      " |  \n",
      " |  adafactor = False\n",
      " |  \n",
      " |  adam_beta1 = 0.9\n",
      " |  \n",
      " |  adam_beta2 = 0.999\n",
      " |  \n",
      " |  adam_epsilon = 1e-08\n",
      " |  \n",
      " |  auto_find_batch_size = False\n",
      " |  \n",
      " |  bf16 = False\n",
      " |  \n",
      " |  bf16_full_eval = False\n",
      " |  \n",
      " |  data_seed = None\n",
      " |  \n",
      " |  dataloader_drop_last = False\n",
      " |  \n",
      " |  dataloader_num_workers = 0\n",
      " |  \n",
      " |  dataloader_pin_memory = True\n",
      " |  \n",
      " |  ddp_bucket_cap_mb = None\n",
      " |  \n",
      " |  ddp_find_unused_parameters = None\n",
      " |  \n",
      " |  ddp_timeout = 1800\n",
      " |  \n",
      " |  debug = ''\n",
      " |  \n",
      " |  deepspeed = None\n",
      " |  \n",
      " |  disable_tqdm = None\n",
      " |  \n",
      " |  do_eval = False\n",
      " |  \n",
      " |  do_predict = False\n",
      " |  \n",
      " |  do_train = False\n",
      " |  \n",
      " |  eval_accumulation_steps = None\n",
      " |  \n",
      " |  eval_delay = 0\n",
      " |  \n",
      " |  eval_steps = None\n",
      " |  \n",
      " |  evaluation_strategy = 'no'\n",
      " |  \n",
      " |  fp16 = False\n",
      " |  \n",
      " |  fp16_backend = 'auto'\n",
      " |  \n",
      " |  fp16_full_eval = False\n",
      " |  \n",
      " |  fp16_opt_level = 'O1'\n",
      " |  \n",
      " |  framework = 'pt'\n",
      " |  \n",
      " |  fsdp = ''\n",
      " |  \n",
      " |  fsdp_min_num_params = 0\n",
      " |  \n",
      " |  fsdp_transformer_layer_cls_to_wrap = None\n",
      " |  \n",
      " |  full_determinism = False\n",
      " |  \n",
      " |  gradient_accumulation_steps = 1\n",
      " |  \n",
      " |  gradient_checkpointing = False\n",
      " |  \n",
      " |  greater_is_better = None\n",
      " |  \n",
      " |  group_by_length = False\n",
      " |  \n",
      " |  half_precision_backend = 'auto'\n",
      " |  \n",
      " |  hub_model_id = None\n",
      " |  \n",
      " |  hub_private_repo = False\n",
      " |  \n",
      " |  hub_strategy = 'every_save'\n",
      " |  \n",
      " |  hub_token = None\n",
      " |  \n",
      " |  ignore_data_skip = False\n",
      " |  \n",
      " |  include_inputs_for_metrics = False\n",
      " |  \n",
      " |  jit_mode_eval = False\n",
      " |  \n",
      " |  label_names = None\n",
      " |  \n",
      " |  label_smoothing_factor = 0.0\n",
      " |  \n",
      " |  learning_rate = 5e-05\n",
      " |  \n",
      " |  length_column_name = 'length'\n",
      " |  \n",
      " |  load_best_model_at_end = False\n",
      " |  \n",
      " |  local_rank = -1\n",
      " |  \n",
      " |  log_level = 'passive'\n",
      " |  \n",
      " |  log_level_replica = 'passive'\n",
      " |  \n",
      " |  log_on_each_node = True\n",
      " |  \n",
      " |  logging_dir = None\n",
      " |  \n",
      " |  logging_first_step = False\n",
      " |  \n",
      " |  logging_nan_inf_filter = True\n",
      " |  \n",
      " |  logging_steps = 500\n",
      " |  \n",
      " |  logging_strategy = 'steps'\n",
      " |  \n",
      " |  lr_scheduler_type = 'linear'\n",
      " |  \n",
      " |  max_grad_norm = 1.0\n",
      " |  \n",
      " |  max_steps = -1\n",
      " |  \n",
      " |  metric_for_best_model = None\n",
      " |  \n",
      " |  mp_parameters = ''\n",
      " |  \n",
      " |  no_cuda = False\n",
      " |  \n",
      " |  num_train_epochs = 3.0\n",
      " |  \n",
      " |  optim = 'adamw_hf'\n",
      " |  \n",
      " |  overwrite_output_dir = False\n",
      " |  \n",
      " |  past_index = -1\n",
      " |  \n",
      " |  per_device_eval_batch_size = 8\n",
      " |  \n",
      " |  per_device_train_batch_size = 8\n",
      " |  \n",
      " |  per_gpu_eval_batch_size = None\n",
      " |  \n",
      " |  per_gpu_train_batch_size = None\n",
      " |  \n",
      " |  prediction_loss_only = False\n",
      " |  \n",
      " |  push_to_hub = False\n",
      " |  \n",
      " |  push_to_hub_model_id = None\n",
      " |  \n",
      " |  push_to_hub_organization = None\n",
      " |  \n",
      " |  push_to_hub_token = None\n",
      " |  \n",
      " |  ray_scope = 'last'\n",
      " |  \n",
      " |  remove_unused_columns = True\n",
      " |  \n",
      " |  report_to = None\n",
      " |  \n",
      " |  resume_from_checkpoint = None\n",
      " |  \n",
      " |  run_name = None\n",
      " |  \n",
      " |  save_on_each_node = False\n",
      " |  \n",
      " |  save_steps = 500\n",
      " |  \n",
      " |  save_strategy = 'steps'\n",
      " |  \n",
      " |  save_total_limit = None\n",
      " |  \n",
      " |  seed = 42\n",
      " |  \n",
      " |  sharded_ddp = ''\n",
      " |  \n",
      " |  skip_memory_metrics = True\n",
      " |  \n",
      " |  tf32 = None\n",
      " |  \n",
      " |  torchdynamo = None\n",
      " |  \n",
      " |  tpu_metrics_debug = False\n",
      " |  \n",
      " |  tpu_num_cores = None\n",
      " |  \n",
      " |  use_ipex = False\n",
      " |  \n",
      " |  use_legacy_prediction_loop = False\n",
      " |  \n",
      " |  use_mps_device = False\n",
      " |  \n",
      " |  warmup_ratio = 0.0\n",
      " |  \n",
      " |  warmup_steps = 0\n",
      " |  \n",
      " |  weight_decay = 0.0\n",
      " |  \n",
      " |  xpu_backend = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a024e68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化训练器\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = args,\n",
    "    train_dataset=dataset_train,\n",
    "    eval_dataset=dataset_test,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4ff78119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 07:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7413558959960938,\n",
       " 'eval_accuracy': 0.54,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_runtime': 17.3016,\n",
       " 'eval_samples_per_second': 11.56,\n",
       " 'eval_steps_per_second': 0.405}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评价模型\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "406b5b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 189\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 14:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.362678</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.835165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.368508</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.831461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.355735</td>\n",
       "      <td>0.865000</td>\n",
       "      <td>0.855615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.3308770315987723, metrics={'train_runtime': 889.136, 'train_samples_per_second': 3.374, 'train_steps_per_second': 0.213, 'total_flos': 77083317000000.0, 'train_loss': 0.3308770315987723, 'epoch': 3.0})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "057218d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 200\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/chenli/.cache/huggingface/modules/datasets_modules/metrics/glue/91f3cfc5498873918ecf119dbf806fb10815786c84f41b85a5d3c47c1519b343 (last modified on Mon Nov  7 19:47:24 2022) since it couldn't be found locally at glue, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3557354807853699,\n",
       " 'eval_accuracy': 0.865,\n",
       " 'eval_f1': 0.8556149732620322,\n",
       " 'eval_runtime': 117.9623,\n",
       " 'eval_samples_per_second': 1.695,\n",
       " 'eval_steps_per_second': 0.059,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评价模型\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23507022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output_dir\n",
      "Configuration saved in ./output_dir/config.json\n",
      "Model weights saved in ./output_dir/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# 保存模型参数\n",
    "# 保存的是模型参数，而不是模型本身\n",
    "trainer.save_model(output_dir='./output_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1e0358",
   "metadata": {},
   "source": [
    "## 使用模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0ed4446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(data):\n",
    "    label = [i['label'] for i in data]\n",
    "    input_ids = [i['input_ids'] for i in data]\n",
    "    token_type_ids = [i['token_type_ids'] for i in data]\n",
    "    attention_mask = [i['attention_mask'] for i in data]\n",
    "    \n",
    "    label = torch.LongTensor(label)\n",
    "    input_ids = torch.LongTensor(input_ids)\n",
    "    token_type_ids = torch.LongTensor(token_type_ids)\n",
    "    attention_mask = torch.LongTensor(attention_mask)\n",
    "    \n",
    "    return label,input_ids,token_type_ids,attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6507e63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载\n",
    "loader_test = torch.utils.data.DataLoader(dataset=dataset_test,batch_size=4,collate_fn=collate_fn,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "24429e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0, 0, 0]),\n",
       " tensor([[ 101, 1912, 6225, 4023,  778, 8024, 4635, 5682, 1469, 6844, 1394, 1957,\n",
       "          4495, 4500,  511, 5314, 4638,  691,  691, 3683, 6772, 6639, 8024, 7674,\n",
       "          1044, 4692,  677,  749,  100, 4638, 4801, 4669,  511, 3683, 2669, 3249,\n",
       "          4638, 2487, 8024, 2798, 5314,  100, 4801, 4669, 1315,  886, 3221, 1743,\n",
       "          2578,  102],\n",
       "         [ 101, 2940,  741, 6206, 3724, 2145, 2787, 5632, 2346, 6934, 2164, 1168,\n",
       "          1266,  776, 8024, 1045, 6934, 6589, 2218, 4685, 2496,  754,  741, 6589,\n",
       "          8024, 6820, 3300, 2553, 6206, 2940, 1408, 8024, 3173,  743,  671, 3315,\n",
       "           679, 3291, 1962, 1408, 8024,  872,  812, 6821, 3416, 4638, 3302, 1218,\n",
       "          2523,  102],\n",
       "         [ 101, 6963, 3221, 6443, 6432, 4638, 1962, 1557, 8024, 2769,  738,  743,\n",
       "           749,  511, 1377, 3221, 4692,  679, 2743, 8024, 2769, 1968, 4692,  749,\n",
       "           738, 6432, 3766, 1567, 4500,  511, 1511,  511,  511,  511,  511,  511,\n",
       "           511,  511,  511,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0],\n",
       "         [ 101, 1796, 1769,  117, 3302, 1218, 4495, 4263, 4415,  679, 4415,  119,\n",
       "          2791, 7313, 3655, 3191,  679, 1838,  117, 6820, 3300, 2460, 1456,  119,\n",
       "          6983, 2421, 3297, 2208, 3300, 8113, 2399, 1325, 1380,  117, 6598, 3160,\n",
       "          2802, 3221, 2458,  689,  697, 2399,  136,  102,    0,    0,    0,    0,\n",
       "             0,    0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]]),\n",
       " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "          0, 0]]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,(label,input_ids,token_type_ids,attention_mask) in enumerate(loader_test):\n",
    "    break\n",
    "\n",
    "label,input_ids,token_type_ids,attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "04cf9482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试\n",
    "import torch\n",
    "\n",
    "def test():\n",
    "    model.load_state_dict(torch.load('./output_dir/pytorch_model.bin')) # 加载模型\n",
    "    \n",
    "    model.eval() # 把模型切换到运行模式\n",
    "    \n",
    "    # 运算\n",
    "    out = model(input_ids=input_ids,token_type_ids=token_type_ids,attention_mask=attention_mask)\n",
    "    \n",
    "    # logits是一个 [4,2] 的矩阵 -> [4],变成四个结果\n",
    "    out = out['logits'].argmax(dim=1)\n",
    "    \n",
    "    correct = (out==label).sum().item()\n",
    "    \n",
    "    return correct/len(label)\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9bcaf769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f90c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

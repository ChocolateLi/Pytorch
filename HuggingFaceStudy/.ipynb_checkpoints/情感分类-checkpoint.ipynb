{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063e34ad",
   "metadata": {},
   "source": [
    "# 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74786ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenli/anaconda3/envs/pytorch/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration default-13ed2499b3a51032\n",
      "Reusing dataset csv (/home/chenli/.cache/huggingface/datasets/csv/default-13ed2499b3a51032/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9600,\n",
       " ('选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       "  1))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_dataset('csv',data_files='./train.csv',split='train')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "\n",
    "        return text, label\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5958f4b",
   "metadata": {},
   "source": [
    "# 加载tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "205117ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c2584",
   "metadata": {},
   "source": [
    "# 定义批处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9aa6b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=500,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    #input_ids:编码之后的数字，就是编码后的词\n",
    "    #attention_mask:是补零的位置是0,其他位置是1。pad的位置是0,其他位置是1\n",
    "    #token_type_ids：第一个句子和特殊符号的位置是0,第二个句子的位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9163794d",
   "metadata": {},
   "source": [
    "# 定义数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19dcea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 500]),\n",
       " torch.Size([16, 500]),\n",
       " torch.Size([16, 500]),\n",
       " tensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "# 取第一批数据\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e065d794",
   "metadata": {},
   "source": [
    "# 加载BERT中文模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da3894b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 393M/393M [05:18<00:00, 1.29MB/s] \n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 500, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "#加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "#模型试算\n",
    "out = pretrained(input_ids=input_ids,\n",
    "           attention_mask=attention_mask,\n",
    "           token_type_ids=token_type_ids)\n",
    "# 16是batch_size，对应数据中的16句话\n",
    "# 500是数据分词的长度，对数据编码的时候，指定每一句话编码成500个词的长度\n",
    "# 768是词编码的维度，也就是把每一个词编码成768维度的向量\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e6e1b",
   "metadata": {},
   "source": [
    "# 定义下游任务模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70d5d784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 这里只定义了一个全连接网络\n",
    "        # 做n分类的话，只需要把2改成n\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # 计算过程\n",
    "        # 1.先拿预训练模型做一个计算，抽取数据中的特征\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "\n",
    "        # 2.把抽取出来的特征放到全连接网络中，并且这个特征只需要第0个词的特征\n",
    "        # [CLS]用于分类任务，并且出现在bert输出的第 0 index\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbee5d",
   "metadata": {},
   "source": [
    "# 训练下游任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cab3d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chenli/anaconda3/envs/pytorch/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7007757425308228 0.5\n",
      "5 0.6961469650268555 0.5\n",
      "10 0.6280856132507324 0.75\n",
      "15 0.6409363746643066 0.625\n",
      "20 0.5809353590011597 0.75\n",
      "25 0.5867862701416016 0.75\n",
      "30 0.6139634847640991 0.625\n",
      "35 0.563926100730896 0.6875\n",
      "40 0.5806879997253418 0.75\n",
      "45 0.47617384791374207 1.0\n",
      "50 0.5543134212493896 0.875\n",
      "55 0.45374664664268494 0.9375\n",
      "60 0.44496291875839233 0.9375\n",
      "65 0.4599880874156952 0.875\n",
      "70 0.492148220539093 0.8125\n",
      "75 0.5374161005020142 0.75\n",
      "80 0.47261425852775574 0.875\n",
      "85 0.5308655500411987 0.6875\n",
      "90 0.5169926881790161 0.875\n",
      "95 0.4295022785663605 0.9375\n",
      "100 0.4518877863883972 0.875\n",
      "105 0.5008432865142822 0.8125\n",
      "110 0.4429056644439697 1.0\n",
      "115 0.45786380767822266 0.875\n",
      "120 0.3863544464111328 1.0\n",
      "125 0.4485105276107788 0.875\n",
      "130 0.50847327709198 0.8125\n",
      "135 0.5336365699768066 0.75\n",
      "140 0.517758846282959 0.8125\n",
      "145 0.454900860786438 0.9375\n",
      "150 0.5203752517700195 0.8125\n",
      "155 0.4101777970790863 1.0\n",
      "160 0.5227699279785156 0.8125\n",
      "165 0.44081419706344604 1.0\n",
      "170 0.45648327469825745 0.8125\n",
      "175 0.5407354831695557 0.8125\n",
      "180 0.469036340713501 0.875\n",
      "185 0.4748653769493103 0.9375\n",
      "190 0.5313305258750916 0.8125\n",
      "195 0.4474937617778778 0.875\n",
      "200 0.41920703649520874 0.875\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "#训练\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "        print(i, loss.item(), accuracy)\n",
    "\n",
    "    if i == 300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a645283",
   "metadata": {},
   "source": [
    "在训练的过程中可以发现，正确率已经达到80 90的样子。这个就是使用 BERT 预训练模型抽取特征所展示出来的威力。以往在这种自然语言上的数据集上面，想要做一个文本分类的任务，想要训练到80 90的正确率，这个训练量是非常大的，模型往往很难收敛。但是从这里发现，使用BERT做预训练模型抽取特征，然后再做下游任务的一个迁移学习。可以在非常短的时间内以非常快的速度就能够达到很高的正确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea018d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f422d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc2c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
